{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, DDPG\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "discount_factor = 0.99\n",
    "total_timesteps = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/DQN\"\n",
    "logs_dir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    \n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000,\n",
    "                                         save_path=models_dir,\n",
    "                                         name_prefix='DQN_model',\n",
    "                                         )\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                            best_model_save_path=models_dir,\n",
    "                            log_path = logs_dir,\n",
    "                            eval_freq=100,\n",
    "                            )\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env  = gym.make(\"LunarLander-v2\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN = DQN('MlpPolicy', env, learning_rate=learning_rate, batch_size=batch_size, gamma=discount_factor, verbose=1, tensorboard_log=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DQN.learn(total_timesteps=total_timesteps, callback=callback ,tb_log_name = \"DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initialize the environment\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# # env = gym.make(\"CartPole-v1\")\n",
    "# # Define parameters\n",
    "# learning_rate = 0.001\n",
    "# batch_size = 64\n",
    "# discount_factor = 0.99\n",
    "# total_timesteps = 250000\n",
    "\n",
    "# # # Create a monitor to log results\n",
    "# env = Monitor(env, filename='./Discrete/discrete_lander/')\n",
    "\n",
    "# # Create the DQN model\n",
    "# model = DQN('MlpPolicy', env, learning_rate=learning_rate, batch_size=batch_size, gamma=discount_factor, verbose=1, tensorboard_log=\"./Discrete/dqn_lunar_lander_tensorboard/\")\n",
    "\n",
    "# # Define a callback for saving the model\n",
    "# checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./Discrete/dqn_checkpoints/', name_prefix='dqn_model')\n",
    "\n",
    "# # Train the agent\n",
    "# model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback, tb_log_name=\"first_run\" )\n",
    "\n",
    "# # Save the final model\n",
    "# model.save(\"./Discrete/dqn_lunar_lander\")\n",
    "\n",
    "# # tensorboard --logdir='./Discrete/dqn_lunar_lander_tensorboard/'\n",
    "# # >tensorboard --logdir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the agent\n",
    "# model.learn(total_timesteps=total_timesteps, callback=checkpoint_callback, tb_log_name=\"first_run\")\n",
    "\n",
    "# # Save the final model\n",
    "# model.save(\"./Discrete/dqn_lunar_lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Pygame window\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "env = gym.make('LunarLander-v2', render_mode = 'human')\n",
    "# env = gym.make('CartPole-v1')\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "model_DQN = DQN.load(\"C:\\\\Users\\\\msaee\\\\Desktop\\\\Semester 10\\\\RL\\\\Quizzes\\\\Quiz 3\\\\Material\\\\DQN\\\\models\\\\DQN\\\\best_model.zip\")\n",
    "\n",
    "for episode in range(0, episodes):\n",
    "    state, info = env.reset()\n",
    "    # state = state[0]\n",
    "    terminated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated :\n",
    "        action, _states = model_DQN.predict(state, deterministic=True)\n",
    "        state, reward, terminated , truncated, info  = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(score)\n",
    "\n",
    "\n",
    "# Close the Pygame window\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "discount_factor = 0.99\n",
    "total_timesteps = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/DDPG\"\n",
    "logs_dir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    \n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000,\n",
    "                                         save_path=models_dir,\n",
    "                                         name_prefix='DDPG_model',\n",
    "                                         )\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                            best_model_save_path=models_dir,\n",
    "                            log_path = logs_dir,\n",
    "                            eval_freq=100,\n",
    "                            )\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.4326781e-04,  1.3996991e+00,  4.4878118e-02, -4.9871606e-01,\n",
       "        -5.0680025e-04, -1.0165600e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_continuous = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    ")\n",
    "\n",
    "env_continuous.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the action noise for DDPG\n",
    "n_actions = env_continuous.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DDPG = DDPG('MlpPolicy', env, \n",
    "                learning_rate=learning_rate, \n",
    "                batch_size=batch_size, \n",
    "                gamma=discount_factor, \n",
    "                learning_starts=50000, \n",
    "                verbose=1,\n",
    "                tensorboard_log=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DDPG.learn(total_timesteps=total_timesteps, callback=callback ,tb_log_name = \"DDPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_continuous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Current working directory: c:\\Users\\msaee\\Desktop\\Semester 10\\RL\\Quizzes\\Quiz 3\\Material\\DQN\n"
     ]
    }
   ],
   "source": [
    "episodes = 2\n",
    "env = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    "    render_mode = 'human',\n",
    ")\n",
    "\n",
    "num_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Define the path to the model\n",
    "model_DDPG_path = os.path.join(\"models\", \"DDPG\", \"best_model\")\n",
    "\n",
    "# Load the model \n",
    "try:\n",
    "    model_DDPG = DDPG.load(model_DDPG_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at path: {model_DDPG_path}. Please check the path.\")\n",
    "\n",
    "\n",
    "for episode in range(0, episodes):\n",
    "    state, info = env.reset()\n",
    "    # state = state[0]\n",
    "    terminated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated :\n",
    "        action, _states = model_DDPG.predict(state, deterministic=True)\n",
    "        state, reward, terminated , truncated, info  = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(score)\n",
    "\n",
    "\n",
    "# Close the Pygame window\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---------------- TESTS ----------------###\n",
    "env_continuous.action_space.sample().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quiz3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
