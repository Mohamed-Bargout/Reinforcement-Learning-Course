{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, DDPG\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "discount_factor = 0.99\n",
    "total_timesteps = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/DDPG\"\n",
    "logs_dir = \"logs\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    \n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000,\n",
    "                                         save_path=models_dir,\n",
    "                                         name_prefix='DDPG_model',\n",
    "                                         )\n",
    "\n",
    "eval_callback = EvalCallback(eval_env,\n",
    "                            best_model_save_path=models_dir,\n",
    "                            log_path = logs_dir,\n",
    "                            eval_freq=100,\n",
    "                            )\n",
    "\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00635777,  1.4082228 ,  0.64394754, -0.11990228, -0.00736018,\n",
       "        -0.14586368,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_continuous = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    ")\n",
    "\n",
    "env_continuous.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the action noise for DDPG\n",
    "n_actions = env_continuous.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_DDPG \u001b[38;5;241m=\u001b[39m DDPG(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43menv\u001b[49m, \n\u001b[0;32m      2\u001b[0m                 learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[0;32m      3\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m      4\u001b[0m                 gamma\u001b[38;5;241m=\u001b[39mdiscount_factor, \n\u001b[0;32m      5\u001b[0m                 learning_starts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, \n\u001b[0;32m      6\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      7\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                 tensorboard_log\u001b[38;5;241m=\u001b[39mlogs_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "model_DDPG = DDPG('MlpPolicy', env_continuous, \n",
    "                learning_rate=learning_rate, \n",
    "                batch_size=batch_size, \n",
    "                gamma=discount_factor, \n",
    "                learning_starts=50000, \n",
    "                verbose=1,\n",
    "                device='cuda',\n",
    "                tensorboard_log=logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DDPG.learn(total_timesteps=total_timesteps, callback=callback ,tb_log_name = \"DDPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_continuous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Current working directory: c:\\Users\\msaee\\Desktop\\Semester 10\\RL\\Quizzes\\Quiz 3\\Material\\DQN\n"
     ]
    }
   ],
   "source": [
    "episodes = 2\n",
    "env = gym.make(\n",
    "    'LunarLander-v2',\n",
    "    continuous = True,\n",
    "    render_mode = 'human',\n",
    ")\n",
    "\n",
    "num_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Define the path to the model\n",
    "model_DDPG_path = os.path.join(\"models\", \"DDPG\", \"best_model\")\n",
    "\n",
    "# Load the model \n",
    "try:\n",
    "    model_DDPG = DDPG.load(model_DDPG_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at path: {model_DDPG_path}. Please check the path.\")\n",
    "\n",
    "\n",
    "for episode in range(0, episodes):\n",
    "    state, info = env.reset()\n",
    "    # state = state[0]\n",
    "    terminated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated :\n",
    "        action, _states = model_DDPG.predict(state, deterministic=True)\n",
    "        state, reward, terminated , truncated, info  = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(score)\n",
    "\n",
    "\n",
    "# Close the Pygame window\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quiz3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
