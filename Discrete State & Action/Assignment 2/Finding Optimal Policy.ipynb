{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_width = 4\n",
    "        self.grid_height = 3\n",
    "        self.start_state = (2, 0)\n",
    "        self.goal_states = [(0, 3), (1, 3)]\n",
    "        self.obstacles = [(1, 1)]\n",
    "        self.action_space = ('UP', 'DOWN', 'RIGHT', \"LEFT\")\n",
    "        self.policy = {(0,0):\"DOWN\", (0,1):\"Right\", (0,2):\"RIGHT\",(1,0):\"DOWN\", (1,2):\"UP\", (2,0):\"Right\",(2,1):\"RIGHT\",(2,2):\"RIGHT\", (2,3): \"UP\"}\n",
    "        self.current_state = self.start_state\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state  # Return the state tuple\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.current_state in self.goal_states\n",
    "\n",
    "    def stochastic_action(self, action):\n",
    "        action = action.upper()\n",
    "        x , y = self.current_state\n",
    "        if(y == 2):\n",
    "            pass\n",
    "            random_number = random.random()\n",
    "            if( random_number <= 0.7):\n",
    "                return \"UP\"\n",
    "            else:\n",
    "                return action      \n",
    "        return action\n",
    "    \n",
    "    def next_state(self, action):\n",
    "        action = self.stochastic_action(action)\n",
    "        x , y = self.current_state\n",
    "        # Update state based on action\n",
    "        if action == \"UP\":\n",
    "            new_state = (x - 1, y)\n",
    "        elif action == \"DOWN\":\n",
    "            new_state = (x + 1, y)\n",
    "        elif action == \"RIGHT\":\n",
    "            new_state = (x, y + 1)\n",
    "        elif action == \"LEFT\":\n",
    "            new_state = (x, y - 1)\n",
    "        \n",
    "        if self.is_valid_state(new_state):\n",
    "            return new_state\n",
    "        return self.current_state\n",
    "    \n",
    "    def is_valid_state(self, state):\n",
    "        x, y = state\n",
    "        if(state in self.obstacles):\n",
    "            return False\n",
    "        if (0 <= x < self.grid_height and 0 <= y < self.grid_width):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        action = action.upper()\n",
    "        # Update state based on action\n",
    "        if action in self.action_space:\n",
    "            new_state = self.next_state(action)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        #Info\n",
    "        info = {\"Previous state\":self.current_state, \"Action\":action, \"Next state\": new_state, \"Reward\": 0, \"Done\": False}\n",
    "        \n",
    "        self.current_state = new_state # Update State\n",
    "\n",
    "        # Check for valid state and update\n",
    "        reward = 0\n",
    "        if self.is_terminal():\n",
    "            reward = 1 if self.current_state == (0, 3) else -1  # Goal 1 reward or failure penalty\n",
    "\n",
    "        done = self.is_terminal()\n",
    "        #Update info\n",
    "        info[\"Reward\"] = reward\n",
    "        info[\"Done\"] = done\n",
    "\n",
    "        # Return observation, reward, done, and optionally info dictionary\n",
    "        return self.current_state, reward, done, info  # Empty info for now\n",
    "    \n",
    "    # Quiz 1:\n",
    "\n",
    "    def reward(self, next_state):\n",
    "        if(next_state in self.goal_states):\n",
    "            if(next_state == (0,3)):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "    def next(self, state, action):\n",
    "        action = action.upper()\n",
    "        x , y = state\n",
    "        if action == \"UP\":\n",
    "            new_state = (x - 1, y)\n",
    "        elif action == \"DOWN\":\n",
    "            new_state = (x + 1, y)\n",
    "        elif action == \"RIGHT\":\n",
    "            new_state = (x, y + 1)\n",
    "        elif action == \"LEFT\":\n",
    "            new_state = (x, y - 1)\n",
    "\n",
    "        if self.is_valid_state(new_state):\n",
    "            return new_state\n",
    "        return state\n",
    "\n",
    "        \n",
    "    def possible_actions(self, state, policy):\n",
    "        x , y = state\n",
    "        next_states = {}\n",
    "\n",
    "        next_state = self.next(state, self.policy[state]) # Find the \"next_state\" for the \"action\" required by the policy at \"state\" \n",
    "        next_states[next_state] = 1\n",
    "\n",
    "        if(y == 2):\n",
    "            next_states[next_state] *= 0.3\n",
    "            next_state = self.next(state,\"UP\")\n",
    "            next_states[next_state] = next_states.get(next_state, 0) + 0.7\n",
    "\n",
    "        return next_states\n",
    "    \n",
    "\n",
    "    def possible_actions_random(self, state):\n",
    "        x , y = state\n",
    "        next_states = {}\n",
    "\n",
    "        for action in self.action_space:\n",
    "            next_state = self.next(state, action)\n",
    "            next_states[next_state] = next_states.get(next_state, 0) + 0.25\n",
    "\n",
    "        return next_states\n",
    "\n",
    "    def policy_evaluation(self, policy ,tol = 0.05, gamma = 0.9 ):\n",
    "        value_function = {(0,0):0, (0,1):0, (0,2):0, (0,3):0, (1,0):0, (1,2):0, (1,3):0, (2,0):0, (2,1):0, (2,2):0, (2,3):0}\n",
    "        value_function_updated = dict(value_function)\n",
    "        \n",
    "        while(1):\n",
    "            delta = 0\n",
    "            value_function = dict(value_function_updated) # Update Value Functions\n",
    "            for state in value_function:\n",
    "                if(state in self.goal_states):\n",
    "                    value_function_updated[state] = 0\n",
    "                    continue\n",
    "                temp = 0\n",
    "                next_states = self.possible_actions(state, policy) # Uncomment for policy given by self.policy\n",
    "                # next_states = self.possible_actions_random(state) # Uncomment for random policy -> (\"UP\": 0.25, \"DOWN\": 0.25, \"RIGHT\": 0.25, \"LEFT\":0.25) # Note: Value functions for this policy matches Assignment 1\n",
    "                for next_state in next_states:\n",
    "                    temp += next_states[next_state]*(self.reward(next_state) + gamma*value_function[next_state])\n",
    "                value_function_updated[state] = temp\n",
    "            \n",
    "            # Calculate delta \n",
    "            difference = {key: abs(value_function[key] - value_function_updated.get(key, 0)) for key in value_function} # Difference between prev and current V(s)\n",
    "            delta = max(max(difference.values()), 0) # Find max value in the difference and return the max(max,0)\n",
    "            if(delta < tol): # if change < tolerance -> END\n",
    "                break\n",
    "                \n",
    "        return value_function_updated\n",
    "    \n",
    "    def possible_actions_action(self, state, action):\n",
    "        x , y = state\n",
    "        next_states = {}\n",
    "\n",
    "        next_state = self.next(state, action) # Find the \"next_state\" for the \"action\" required by the policy at \"state\" \n",
    "        next_states[next_state] = 1\n",
    "\n",
    "        if(y == 2):\n",
    "            next_states[next_state] *= 0.3\n",
    "            next_state = self.next(state,\"UP\")\n",
    "            next_states[next_state] = next_states.get(next_state, 0) + 0.7\n",
    "\n",
    "        return next_states\n",
    "    \n",
    "    def actionValue(self, value_function, state, action, gamma = 0.9):\n",
    "        next_states = self.possible_actions_action(state,action) # Returns {(next_state1): its probability, (next_state2): its probability}\n",
    "        # Action value = probability_of_next_state1*(reward + gamma*(V(next_state1))) + ... \n",
    "        action_value = 0\n",
    "        for next_state, probability in next_states.items():\n",
    "            action_value += probability*(self.reward(next_state) + gamma*value_function[next_state])\n",
    "\n",
    "        return action_value\n",
    "    \n",
    "    def actionValue_functions(self, policy, value_func, gamma = 0.9):\n",
    "        action_value = {\"UP\":0 , \"DOWN\":0 , \"RIGHT\":0 , \"LEFT\":0}\n",
    "        action_value_functions = {} \n",
    "        for state in policy:\n",
    "            for action in action_value:\n",
    "                action_value[action] = self.actionValue(value_func, state, action, gamma)\n",
    "                \n",
    "            action_value_functions[state] = dict(action_value)\n",
    "\n",
    "        return action_value_functions\n",
    "    \n",
    "    def policy_improvement(self, policy, tol = 0.05, gamma = 0.9):\n",
    "\n",
    "        policy_stable = False\n",
    "        while(not policy_stable):\n",
    "            # step 1: Policy Evaluation\n",
    "            value_func = self.policy_evaluation(policy, tol = tol, gamma = gamma)\n",
    "\n",
    "            # Step 2: Policy Improvement\n",
    "            policy_stable = True\n",
    "            action_value = {\"UP\":0 , \"DOWN\":0 , \"RIGHT\":0 , \"LEFT\":0}\n",
    "            action_value_functions = {} \n",
    "            for state in policy:\n",
    "                for action in action_value:\n",
    "                    action_value[action] = self.actionValue(value_func, state, action, gamma = gamma)\n",
    "                    \n",
    "                best_action = max(action_value, key=action_value.get)\n",
    "                if(best_action != policy[state]):\n",
    "                    policy_stable = False\n",
    "                    policy[state] = best_action\n",
    "        return policy\n",
    "        \n",
    "def Display(action_value_function):\n",
    "    print(\"State  : [ UP   | DOWN  | RIGHT  | LEFT ]\")\n",
    "    for state,dic in action_value_function.items():\n",
    "        up = dic[\"UP\"]\n",
    "        down = dic[\"DOWN\"]\n",
    "        right = dic[\"RIGHT\"]\n",
    "        left = dic[\"LEFT\"]\n",
    "        print(f\"{state} : [{up} , {down}, {right}, {left}] \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 'RIGHT',\n",
       " (0, 1): 'RIGHT',\n",
       " (0, 2): 'RIGHT',\n",
       " (1, 0): 'UP',\n",
       " (1, 2): 'UP',\n",
       " (2, 0): 'UP',\n",
       " (2, 1): 'RIGHT',\n",
       " (2, 2): 'UP',\n",
       " (2, 3): 'LEFT'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Best Policy:\n",
    "gw = GridWorld()\n",
    "best_policy = gw.policy_improvement(gw.policy, tol = 0.000001, gamma = 0.9)\n",
    "display(best_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6568 0.7297 0.8108 0.    ]\n",
      " [0.5911 0.     0.7297 0.    ]\n",
      " [0.532  0.5911 0.6568 0.5911]]\n"
     ]
    }
   ],
   "source": [
    "# State Value Function of Best Policy:\n",
    "state_value_function = gw.policy_evaluation(best_policy, tol=0.000001, gamma=0.9) # (self, policy ,tol = 0.05, gamma = 0.9 ):\n",
    "state_value_function = {key: round(value, 4) for key, value in state_value_function.items()} # Round\n",
    "state_value_function = [ [state_value_function.get((x,y), 0) for y in range(gw.grid_width )] for x in range(gw.grid_height)] # Place in 2D array\n",
    "state_value_function = np.array(state_value_function)\n",
    "print(state_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State  : [ UP   | DOWN  | RIGHT  | LEFT ]\n",
      "(0, 0) : [0.5911 , 0.532, 0.6567, 0.5911] \n",
      "(0, 1) : [0.6567 , 0.6567, 0.7297, 0.5911] \n",
      "(0, 2) : [0.7297 , 0.7078, 0.8108, 0.7078] \n",
      "(1, 0) : [0.5911 , 0.4788, 0.532, 0.532] \n",
      "(1, 2) : [0.7297 , 0.6881, 0.2108, 0.7078] \n",
      "(2, 0) : [0.532 , 0.4788, 0.532, 0.4788] \n",
      "(2, 1) : [0.532 , 0.532, 0.5911, 0.4788] \n",
      "(2, 2) : [0.6567 , 0.637, 0.6193, 0.6193] \n",
      "(2, 3) : [-1.0 , 0.532, 0.532, 0.5911] \n",
      "(0, 3) : [0 , 0, 0, 0] \n",
      "(1, 3) : [0 , 0, 0, 0] \n"
     ]
    }
   ],
   "source": [
    "# Action Value Function of Best Policy:\n",
    "action_value_function =  gw.actionValue_functions(best_policy, state_value_function, gamma=0.9) #def actionValue_functions(self, policy, value_func, gamma = 0.9):\n",
    "action_value_function[(0,3)] = {\"UP\": 0 , \"DOWN\": 0 , \"RIGHT\": 0, \"LEFT\": 0}\n",
    "action_value_function[(1,3)] = {\"UP\": 0 , \"DOWN\": 0 , \"RIGHT\": 0, \"LEFT\": 0}\n",
    "for key in action_value_function: # key = (0,0)\n",
    "    for action in action_value_function[key]: # action = \"UP\"\n",
    "        dic = action_value_function[key] \n",
    "        dic[action]= round(dic[action], 4)\n",
    "\n",
    "Display(action_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([(2, 0), 'UP', (1, 0), 0, False])\n",
      "dict_values([(1, 0), 'UP', (0, 0), 0, False])\n",
      "dict_values([(0, 0), 'RIGHT', (0, 1), 0, False])\n",
      "dict_values([(0, 1), 'RIGHT', (0, 2), 0, False])\n",
      "dict_values([(0, 2), 'RIGHT', (0, 2), 0, False])\n",
      "dict_values([(0, 2), 'RIGHT', (0, 2), 0, False])\n",
      "dict_values([(0, 2), 'RIGHT', (0, 3), 1, True])\n"
     ]
    }
   ],
   "source": [
    "# Start to Finish using Best policy\n",
    "gw.reset()\n",
    "gw.current_state = (2,0) # Change Start state\n",
    "done = gw.is_terminal()\n",
    "while not done:\n",
    "    action = best_policy[gw.current_state]\n",
    "    new_state, reward, done, info = gw.step(action)\n",
    "    print(info.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 3): 0.3, (0, 2): 0.7}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw = GridWorld() \n",
    "# policy = {(0,0):\"DOWN\", (0,1):\"Right\", (0,2):\"RIGHT\",(1,0):\"DOWN\", (1,2):\"UP\", (2,0):\"Right\",(2,1):\"RIGHT\",(2,2):\"RIGHT\", (2,3): \"UP\"}\n",
    "gw.possible_actions((0,2), gw.policy) # Returns possible next states and their probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1245  0.7297  0.8108  0.    ]\n",
      " [ 0.1383  0.      0.7297  0.    ]\n",
      " [ 0.1537  0.1708  0.1897 -1.    ]]\n",
      "{(0, 0): 0.1245, (0, 1): 0.7297, (0, 2): 0.8108, (0, 3): 0, (1, 0): 0.1383, (1, 2): 0.7297, (1, 3): 0, (2, 0): 0.1537, (2, 1): 0.1708, (2, 2): 0.1897, (2, 3): -1.0}\n"
     ]
    }
   ],
   "source": [
    "# Test Policy Evaluation: Uncomment one of the two options for policy\n",
    "gw = GridWorld()\n",
    "dic = gw.policy_evaluation(gw.policy, tol = 0.000001, gamma = 0.9)\n",
    "\n",
    "rounded_dict = {key: round(value, 4) for key, value in dic.items()} # Round\n",
    "\n",
    "array2d = [ [rounded_dict.get((x,y), 0) for y in range(gw.grid_width )] for x in range(gw.grid_height)] # Place in 2D array\n",
    "array2d = np.array(array2d)\n",
    "\n",
    "print(array2d)\n",
    "print(rounded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1245  0.7297  0.8108  0.    ]\n",
      " [ 0.1383  0.      0.7297  0.    ]\n",
      " [ 0.1537  0.1708  0.1897 -1.    ]]\n",
      "0.17073000000000002\n",
      "{(0, 0): {'UP': 0.11205, 'DOWN': 0.12447000000000001, 'RIGHT': 0.65673, 'LEFT': 0.11205}, (0, 1): {'UP': 0.65673, 'DOWN': 0.65673, 'RIGHT': 0.72972, 'LEFT': 0.11205}, (0, 2): {'UP': 0.72972, 'DOWN': 0.7078230000000001, 'RIGHT': 0.8108040000000001, 'LEFT': 0.7078230000000001}, (1, 0): {'UP': 0.11205, 'DOWN': 0.13833, 'RIGHT': 0.12447000000000001, 'LEFT': 0.12447000000000001}, (1, 2): {'UP': 0.72972, 'DOWN': 0.562023, 'RIGHT': 0.21080400000000005, 'LEFT': 0.7078230000000001}, (2, 0): {'UP': 0.12447000000000001, 'DOWN': 0.13833, 'RIGHT': 0.15372000000000002, 'LEFT': 0.13833}, (2, 1): {'UP': 0.15372000000000002, 'DOWN': 0.15372000000000002, 'RIGHT': 0.17073000000000002, 'LEFT': 0.13833}, (2, 2): {'UP': 0.65673, 'DOWN': 0.51093, 'RIGHT': 0.18971099999999996, 'LEFT': 0.505827}, (2, 3): {'UP': -1.0, 'DOWN': -0.9, 'RIGHT': -0.9, 'LEFT': 0.17073000000000002}}\n",
      "{(0, 0): 'RIGHT', (0, 1): 'RIGHT', (0, 2): 'RIGHT', (1, 0): 'UP', (1, 2): 'UP', (2, 0): 'UP', (2, 1): 'RIGHT', (2, 2): 'UP', (2, 3): 'LEFT'}\n"
     ]
    }
   ],
   "source": [
    "# Test Action Value: actionValue(self, value_function, state, action, gamma = 0.9):\n",
    "print(array2d)\n",
    "action_value = gw.actionValue(rounded_dict, (2,3), \"LEFT\", gamma=0.9)\n",
    "print(action_value)\n",
    "\n",
    "# Test actionValue_functions: actionValue_functions(self, policy, value_func, gamma = 0.9):\n",
    "gw = GridWorld()\n",
    "dic = gw.policy_evaluation(gw.policy, tol = 0.000001, gamma = 0.9)\n",
    "value_function = {key: round(value, 4) for key, value in dic.items()} # Round\n",
    "\n",
    "dic = gw.actionValue_functions(gw.policy, value_function, gamma = 0.9 )\n",
    "print(dic)\n",
    "\n",
    "# Test Policy Improvement\n",
    "best_policy = gw.policy_improvement(gw.policy, tol = 0.000001, gamma = 0.9)\n",
    "print(best_policy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
